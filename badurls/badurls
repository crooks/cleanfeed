#!/usr/bin/python
#
# vim: tabstop=4 expandtab shiftwidth=4 autoindent
#
# template.py -- This is a template for new Python programs
#
# Copyright (C) 2005 Steve Crook <steve@mixmin.org>
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTIBILITY
# or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
# for more details.

import config
from operator import itemgetter
from time import time
from datetime import datetime
import os.path
import re
import sys
import shelve

def utcnow():
    """Just return the utc time.  Everything should work on utc."""
    utctime = datetime.utcnow()
    utcstamp = utctime.strftime("%Y-%m-%d %H:%M:%S")
    return utcstamp

def SecsFromEpoch():
    """Return the whole number of seconds since Epoch"""
    return int(time())

def BackoffInterval(then, now):
    """How much to decrement each entry in the DB by."""
    backoff = int((now - then) / config.backoff_interval)
    backoff = backoff * config.backoff_rate
    print "Last run was %d mins ago." % ((now - then) / 60, )
    print "Backoff is %d" % (backoff, )
    return backoff

def Dict2List(dict):
    """Take a dictionary and return a list sorted numerically by value."""
    return sorted(dict.iteritems(), key=itemgetter(1), reverse=True)

def File2List(filename):
    """Read a file and return each line as a list item."""
    items = []
    readlist = open(filename, 'r')
    for line in readlist:
        if not line.startswith('#') and len(line) > 1:
            items.append(line.rstrip())
    return items

def Var2File(filename, var):
    """Write a single variable to a file"""
    file = open(filename, 'w')
    file.write(var)
    file.close()

def IsFile(filename):
    """Return True is a passed filename exists and is a normal file."""
    if os.path.isfile(filename):
        return True
    sys.stdout.write("%s: Not a valid file\n" % (filename,))
    return False

def GetSize(filename):
    """Return the size of a given file in Bytes"""
    return os.path.getsize(filename)

def Excluded(exclude, entry):
    """Compare a passed entry with all the entries in our exclude file.
    Entries will be treated as plain text unless wrapped within slashes in
    which case they will be regarded as regex's."""
    excluded = False
    for rule in exclude:
        # Entries wrapped in /.../ are regex format.
        if rule.startswith('/') and rule.endswith('/'):
            # Strip the first and last chars.
            rule = rule[1:-1]
            if re.search(rule, entry, I):
                excluded = True
                break
        # If entry is not regex, do a plain-text compare.
        else:
            lcentry = entry.lower()
            if lcentry.find(rule) >= 0:
                excluded = True
                break
    return excluded

def UpdateHits(previous, current, count):
    """Use the current count (hits for a given url) to update the stored DB
    value.  As the logfiles rotate daily, we make the assumption that a current
    count of less than a previous implies that logs have rotated."""
    # now is greater than before so increment current with their difference.
    if count > previous:
        current += (count - previous)
    # count is less than before, we assume log rotation has occured.
    elif count < previous:
        current += count
    # We return count as previous because we want previous to become the
    # current count for the next run.
    previous = count
    if current > config.backoff_ceiling:
        current = config.backoff_ceiling
    return previous, current

def RegexSafe(regex):
    """Manipulate a passed fqdn to make it regex friendly"""
    regex = regex.lower()
    regex = regex.replace('.', '\.')
    regex = regex.replace('-', '\-')
    # This should never happen but best to be careful.
    regex = regex.replace('||', '|')
    return regex

def SetBackoff():
    """The seconds since epoch of when a last run occured.  It's used to
    calculate the backoff rate for decrementing hit counts against each URL."""
    # Get the last run time from our persistent database.
    if 'lastrun' in vars:
        lastrun = vars['lastrun']
    else:
        # If we've never been run before, fudge the last run time.
        lastrun = SecsFromEpoch() - config.backoff_interval
    backoff = BackoffInterval(lastrun, SecsFromEpoch())
    # We only want to reset the lastrun time if backoff is greater than zero,
    # otherwise repeated runs more frequently than backoff_interval will result
    # in constant lastrun resets without any backoff happening.
    if backoff > 0:
        vars['lastrun'] = SecsFromEpoch()
    return backoff

def GetURLs(files):
    """Read a list of files.  Return an occurances count of each individual
    URL contained within those files."""
    hits = {}
    regex = re.compile('(?:http:\/\/(?:www\.)?|www\.)([\w\.\-]{1,70})', re.I)
    for filename in files:
        if IsFile(filename):
            print "Scanning", filename
            #filesize = GetSize(filename)
            file = open(filename, 'r')
            onbody = False
            for line in file:
                line = line.strip()
                # All our logfiles contain cutmarks to separate headers from
                # bodies.
                if line == '----- End Message Headers -----' and not onbody:
                    onbody = True
                    continue
                if line == '----- Begin Message Headers -----' and onbody:
                    onbody = False
                    continue
                # We only want to process message bodies, not headers.
                if not onbody:
                    continue
                test = regex.search(line)
                if test:
                    url = test.group(1)
                    url = url.lower()
                    if url in hits:
                        if hits[url] < config.backoff_ceiling:
                            hits[url] += 1
                    else:
                        hits[url] = 1
            file.close()
    return hits

def Main():
    # Logfiles to be read by default.  This can be overridden by command line args.
    logfiles = File2List(config.filelist)
    # URL's to exclude from processing.  Entries can be plain-text or regex.
    exclude = File2List(config.exclude)

    # The regex to match entries against.
    regex = re.compile('(?:http:\/\/(?:www\.)?|www\.)([\w\.\-]{1,70})', re.I)

    # Open the shelves
    global vars
    vars = shelve.open(config.varfile)
    db = shelve.open(config.dbfile)

    # Set the backoff rate
    backoff = SetBackoff()

    hits = GetURLs(logfiles)

    for hit in hits:
        count = hits[hit]
        if hit in db:
            previous, current = db[hit]
            # UpdateHits returns previous and current
            db[hit] = UpdateHits(previous, current, count)
        else:
            # If this is a new URL, set previous and current to the newly
            # discovered count.
            db[hit] = [count, count]

    # Prepare to overwrite Cleanfeed's bad_url_central file.
    badurl = open(config.cfbadurl, 'w')
    badurl.write('######################################################\n')
    badurl.write('# This file is downloaded from a central resource.   #\n')
    badurl.write('# Do not manually edit it as your changes will be    #\n')
    badurl.write('# overwritten during the next scheduled download.    #\n')
    badurl.write('# For manual listings, update the bad_url file.      #\n')
    badurl.write('######################################################\n\n')
    badurl.write('# Last Updated: %s\n\n' % (utcnow(),))

    for url in db:
        p, c = db[url]
        if backoff > 0:
            c = c - backoff
            p = p - backoff
            if c <= 0:
                del db[url]
            else:
                db[url] = [p, c]
        if not Excluded(exclude, url) and c > config.threshold:
            badurl.write(RegexSafe(url) + "\n")
            print p, c, url
    # Close the file and shelves
    badurl.close()
    vars.close()
    db.close()

if (__name__ == "__main__"):
    Main()
